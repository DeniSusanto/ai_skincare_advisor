{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score a new selfie image and evaluate the acne severity of the face\n",
    "Given a set of selfie images, this script is used to score the acne severity levels of the selfie face images. We score each skin patch from the selfie and the average score is chosen as the image label. \n",
    "\n",
    "The scoring pipelines have the following four steps:\n",
    "1. Extract skin patches\n",
    "2. Predict the label for each skin patch\n",
    " *  Extract features from CNTK pretrained model\n",
    " *  Score the features using the trained full connected neural network model\n",
    "3. infer the whole face label based on predicted labels of the skin patches from that selfie image. The inferred labels of the entire selfie images in the test directory are output to a csv file. \n",
    "4. (Optional) If you also have a csv file with the ground truth labels of the test images, we also compare the predicted label and the ground truth labels, and calculate the RMSE on the golden set. \n",
    "\n",
    "***Note***: With the model you trained on Step 3, where random seed = 5 for splitting the training images into training and validation, and initializing the weights of the neural network models, you should get ***RMSE = 0.4819*** on golden set images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "### Test images\n",
    "- Test images should be put in a directory on the machine, and provide the path to this directory to parameter image\\_path.\n",
    "- (***Optional***) Ground truth labels of the test images should be put in a csv file with a headerline, and two columns: Image_Name, Ground_Truth. If this file is None, calculating the RMSE on the test images will be skipped.\n",
    "\n",
    "### Python, Python libraries and self-defined Python Script\n",
    "- Python 3.5 or later version \n",
    "- CNTK, PIL\n",
    "- getPatches.py to extract patches from a selfie (modified from ***[Step 1. Extract Forehead, cheeks, and chin skin patches from raw images using facial landmark model and One Eye model](../01_DataPrep/Step 1. Extract Forehead, cheeks, and chin skin patches from raw images using facial landmark model and One Eye model.ipynb)***)\n",
    "***Note*** You need to mofidy the path to the pretrained landmark model and the Cascade Eye model in the getPatches.py file. \n",
    "\n",
    "### Pretrained models\n",
    "- [Frontal face landmark model](https://github.com/AKSHAYUBHAT/TensorFace/blob/master/openface/models/dlib/shape_predictor_68_face_landmarks.dat) in the same directory as this jupyter notebook.\n",
    "- [One Eye model](https://raw.githubusercontent.com/opencv/opencv/master/data/haarcascades/haarcascade_eye.xml) in the same directory as this jupyter notebook.\n",
    "- [ResNet152\\_ImageNet\\_Caffe.model](https://www.cntk.ai/Models/Caffe_Converted/ResNet152_ImageNet_Caffe.model)\n",
    "- Trained full connected neural network regression model from ***[Step 3. Training_Pipeline](Step3_Training_Pipeline.ipynb)***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model_name = 'ResNet152_ImageNet_Caffe.model'\n",
    "pretrained_model_path = '../models'\n",
    "pretrained_node_name = 'pool5' \n",
    "\n",
    "label_mapping = {1: '1-Clear', 2: '2-Almost Clear', 3: '3-Mild', 4: '4-Moderate', 5: '5-Severe'}\n",
    "\n",
    "img_path = '../data/labeled_test_images'\n",
    "test_ground_truth = '../data/test_images_labels.csv' #If None, calculating RMSE on test set will be skipped\n",
    "result_file = '../data/predicted_labels.csv'\n",
    "patch_path = '../data/test_images_patches'\n",
    "regression_model_path = '../models/cntk_regression.dat'\n",
    "eye_cascade_model = '../models/haarcascade_eye.xml'\n",
    "\n",
    "image_height = 224 # Here are the image height and width that the skin patches of the testing selfie are going to be resized to.\n",
    "image_width  = 224 # They have to be the same as the ResNet-152 model requirement.\n",
    "num_channels = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\deni susanto\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\cntk\\cntk_py_init.py:84: UserWarning: \n",
      "\n",
      "################################################ Missing optional dependency (GPU-Specific) ################################################\n",
      "   CNTK may crash if the component that depends on those dependencies is loaded.\n",
      "   Visit https://docs.microsoft.com/en-us/cognitive-toolkit/Setup-Windows-Python#optional-gpu-specific-packages for more information.\n",
      "############################################################################################################################################\n",
      "If you intend to use CNTK without GPU support, you can ignore the (likely) GPU-specific warning!\n",
      "############################################################################################################################################\n",
      "\n",
      "  warnings.warn(WARNING_MSG_GPU_ONLY % ('GPU-Specific', 'https://docs.microsoft.com/en-us/cognitive-toolkit/Setup-Windows-Python#optional-gpu-specific-packages'))\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import join, isfile, splitext\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cntk as C\n",
    "from PIL import Image\n",
    "import pickle\n",
    "import time\n",
    "import json\n",
    "from cntk import load_model, combine\n",
    "import cntk.io.transforms as xforms\n",
    "from cntk.logging import graph\n",
    "from cntk.logging.graph import get_node_outputs\n",
    "import getPatches\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Skin Patches\n",
    "\n",
    "In the scoring pipeline, the first step is to extract forehead, left cheek, right cheek, and chin skin patches from the original selfie images. The skin patch images are going to be saved in the directory specified in variable patch\\_path.\n",
    "\n",
    "During this skin patch extraction process, a facial landmark model will be first applied. If face is detected by this model, skin patches will be extracted. If face is not detected by the landmark model, OneEye model will be applied to detect the location of a single eye. Then, forehead and cheek will be extracted based on the eye location.\n",
    "\n",
    "If neither landmark nor the OneEye model works, the entire selfie will be used to make predictions.\n",
    "\n",
    "Landmark needs to see both eyes open from the camera. OneEye model works if there is only one open eye identified. Skin patches can be extracted more accurately based on landmark model. So, if possible, we encourage users to face the camera directly with both eyes opened. \n",
    "\n",
    "Keep in mind, depending on the angle of the face facing the camera, this step might result in 1, 2, 3, or 4 skin patch images. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the dimension of each patch of images in the testing image directory\n",
    "dimension_dict = dict()\n",
    "imageFiles = [f for f in listdir(img_path) if isfile(join(img_path, f))]\n",
    "for imagefile in imageFiles:\n",
    "    dim = getPatches.extract_patches(join(img_path, imagefile), {}, {}, patch_path) #extract_patches function is defined in getPatches.py\n",
    "    dimension_dict[imagefile] = dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Image Patch Label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load CNTK Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the pretrained model is ResNet152_ImageNet_Caffe.model\n",
      "the selected layer name is pool5 and the number of flatten nodes is 2048\n"
     ]
    }
   ],
   "source": [
    "# define pretrained model location, node name\n",
    "model_file  = os.path.join(pretrained_model_path, pretrained_model_name)\n",
    "loaded_model  = load_model(model_file)\n",
    "node_in_graph = loaded_model.find_by_name(pretrained_node_name)\n",
    "output_nodes  = combine([node_in_graph.owner])\n",
    "\n",
    "node_outputs = C.logging.get_node_outputs(loaded_model)\n",
    "for l in node_outputs: \n",
    "    if l.name == pretrained_node_name:\n",
    "        num_nodes = np.prod(np.array(l.shape))\n",
    "        \n",
    "print ('the pretrained model is %s' % pretrained_model_name)\n",
    "print ('the selected layer name is %s and the number of flatten nodes is %d' % (pretrained_node_name, num_nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(image_path):   \n",
    "    img = Image.open(image_path)       \n",
    "    resized = img.resize((image_width, image_height), Image.ANTIALIAS)  \n",
    "    \n",
    "    bgr_image = np.asarray(resized, dtype=np.float32)[..., [2, 1, 0]]    \n",
    "    hwc_format = np.ascontiguousarray(np.rollaxis(bgr_image, 2)) \n",
    "    \n",
    "    arguments = {loaded_model.arguments[0]: [hwc_format]}    \n",
    "    output = output_nodes.eval(arguments)   \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load NN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the stored regression model\n",
    "read_model = pd.read_pickle(regression_model_path)\n",
    "regression_model = read_model['model'][0]\n",
    "train_regression = pickle.loads(regression_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the score value for each patch\n",
    "patch_score = dict()\n",
    "for file in next(os.walk(patch_path))[2]:\n",
    "    file_path = os.path.join(patch_path, file)\n",
    "    # extract features from CNTK pretrained model\n",
    "    score_features = extract_features (file_path)[0].flatten()\n",
    "    # score the extracted features using trained regression model\n",
    "    pred_score_label = train_regression.predict(score_features.reshape(1,-1))\n",
    "    patch_score[file] = float(\"{0:.2f}\".format(pred_score_label[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the predicted labels of skin patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'20171226_203858_landmark_chin.jpg': 1.9,\n",
       " '20171226_203858_landmark_fh.jpg': 1.02,\n",
       " '20171226_203858_landmark_lc.jpg': 1.01,\n",
       " '20171226_203858_landmark_rc.jpg': 1.02,\n",
       " '20180727_203313_landmark_chin.jpg': 1.01,\n",
       " '20180727_203313_landmark_fh.jpg': 1.04,\n",
       " '20180727_203313_landmark_lc.jpg': 2.11,\n",
       " '20180727_203313_landmark_rc.jpg': 1.01,\n",
       " '20190530_164944_landmark_chin.jpg': 1.01,\n",
       " '20190530_164944_landmark_fh.jpg': 1.0,\n",
       " '20190530_164944_landmark_lc.jpg': 1.0,\n",
       " '20190530_164944_landmark_rc.jpg': 1.0,\n",
       " '20200108_084050_landmark_chin.jpg': 2.26,\n",
       " '20200108_084050_landmark_fh.jpg': 2.97,\n",
       " '20200108_084050_landmark_lc.jpg': 1.6,\n",
       " '20200108_084050_landmark_rc.jpg': 2.09,\n",
       " 'IMG_20170412_185554_landmark_chin.jpg': 1.4,\n",
       " 'IMG_20170412_185554_landmark_fh.jpg': 2.12,\n",
       " 'IMG_20170412_185554_landmark_rc.jpg': 2.6,\n",
       " 'jongun_landmark_chin.jpg': 1.0,\n",
       " 'jongun_landmark_fh.jpg': 1.02,\n",
       " 'jongun_landmark_lc.jpg': 1.0,\n",
       " 'jongun_landmark_rc.jpg': 1.01,\n",
       " 'me1_landmark_chin.jpg': 1.87,\n",
       " 'me1_landmark_fh.jpg': 1.01,\n",
       " 'me1_landmark_lc.jpg': 1.01,\n",
       " 'me1_landmark_rc.jpg': 1.02,\n",
       " 'set1_clean_landmark_chin.jpg': 1.62,\n",
       " 'set1_clean_landmark_fh.jpg': 1.79,\n",
       " 'set1_clean_landmark_lc.jpg': 1.01,\n",
       " 'set1_clean_landmark_rc.jpg': 1.09,\n",
       " 'set1_landmark_chin.jpg': 2.77,\n",
       " 'set1_landmark_fh.jpg': 5.42,\n",
       " 'set1_landmark_lc.jpg': 2.17,\n",
       " 'set1_landmark_rc.jpg': 3.26,\n",
       " 'set2_clean_landmark_chin.jpg': 1.01,\n",
       " 'set2_clean_landmark_fh.jpg': 1.03,\n",
       " 'set2_clean_landmark_lc.jpg': 1.02,\n",
       " 'set2_clean_landmark_rc.jpg': 1.32,\n",
       " 'set2_landmark_chin.jpg': 1.93,\n",
       " 'set2_landmark_fh.jpg': 2.96,\n",
       " 'set2_landmark_lc.jpg': 1.8,\n",
       " 'set2_landmark_rc.jpg': 2.05,\n",
       " 'Soyeon Choi_landmark_chin.jpg': 1.01,\n",
       " 'Soyeon Choi_landmark_fh.jpg': 1.02,\n",
       " 'Soyeon Choi_landmark_lc.jpg': 1.02,\n",
       " 'Soyeon Choi_landmark_rc.jpg': 1.01,\n",
       " 'soyeon_landmark_chin.jpg': 0.99,\n",
       " 'soyeon_landmark_fh.jpg': 1.02,\n",
       " 'soyeon_landmark_lc.jpg': 1.01,\n",
       " 'soyeon_landmark_rc.jpg': 1.01}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patch_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Infer Image Label\n",
    "\n",
    "After each skin patch image is predicted, we need to infer the predicted label of the entire selfie from the predicted skin patch labels. In this application, we choose to maximal predicted skin patch label as the predicted label of the entire selfie. This logic can be modified further to balance the performance of the model on the entire 5 levels of the acne severity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the max score value among the patches and record the image name\n",
    "image_patch_scores = {}\n",
    "for key in patch_score:\n",
    "    image_id = key.split(\"_landmark\")[0]\n",
    "    image_patch_scores_i = image_patch_scores.get(image_id, {\"patch_name\":[], \"patch_score\":[]})\n",
    "    image_patch_scores_i[\"patch_name\"].append(key)\n",
    "    image_patch_scores_i[\"patch_score\"].append(patch_score[key])\n",
    "    image_patch_scores[image_id] = image_patch_scores_i\n",
    "\n",
    "fp = open(result_file, 'w')\n",
    "fp.write(\"Image_Name, Predicted_Label_Avg, Most_Severe_Patch\\n\")\n",
    "\n",
    "for key in image_patch_scores:\n",
    "    image_name = key.split(\"_landmark\")[0]\n",
    "    max_index = np.argmax(image_patch_scores[key]['patch_score'])\n",
    "    Predicted_Label_Avg = np.mean(image_patch_scores[key]['patch_score'])\n",
    "    Most_Severe_Patch = image_patch_scores[key]['patch_name'][max_index]\n",
    "    fp.write('%s, %.4f, %s\\n'%(image_name, Predicted_Label_Avg, Most_Severe_Patch))\n",
    "\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Calculate RMSE on Test Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE=0.5649\n"
     ]
    }
   ],
   "source": [
    "conditions = [\"0-Not Acne\", \"1-Clear\", \"2-Almost Clear\", \"3-Mild\", \"4-Moderate\", \"5-Severe\"]\n",
    "if test_ground_truth is not None:\n",
    "    fp = open(test_ground_truth, 'r')\n",
    "    fp.readline()\n",
    "    ground_truth = {}\n",
    "    for row in fp:\n",
    "        row = row.strip().split(',')\n",
    "        ground_truth[row[0]] = float(conditions.index(row[1]))\n",
    "    fp.close()\n",
    "    \n",
    "    fp2 = open(result_file, 'r')\n",
    "    fp2.readline()\n",
    "    num_images = 0.0\n",
    "    RMSE = 0.0\n",
    "    for row in fp2:\n",
    "        row = row.strip().split(',')\n",
    "        try:\n",
    "            RMSE += (ground_truth[row[0].split(\"_landmark\")[0]+\".jpg\"] - float(row[1]))**2\n",
    "        except:\n",
    "            RMSE += (ground_truth[row[0].split(\"_landmark\")[0]+\".png\"] - float(row[1]))**2\n",
    "        num_images += 1.0\n",
    "    fp2.close()\n",
    "    RMSE = np.sqrt(RMSE/num_images)\n",
    "print(\"RMSE=%.4f\"%RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'me1'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'20171226_203858.jpg': 1.0,\n",
       " '20180727_203313.jpg': 1.0,\n",
       " '20190530_164944.jpg': 1.0,\n",
       " '20200108_084050.jpg': 3.0,\n",
       " 'IMG_20170412_185554.jpg': 2.0,\n",
       " 'jongun.jpg': 1.0,\n",
       " 'me1.png': 1.0,\n",
       " 'set1.jpg': 5.0,\n",
       " 'set1_clean.jpg': 1.0,\n",
       " 'set2.jpg': 3.0,\n",
       " 'set2_clean.jpg': 1.0,\n",
       " 'Soyeon Choi.jpg': 1.0,\n",
       " 'soyeon.jpg': 1.0}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
